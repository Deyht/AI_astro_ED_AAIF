{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **CAT Faces Generator - Denoising Diffusion Probabilistic Model**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deyht/AI_astro_ED_AAIF/blob/main/codes/CNN/diffusion/cat_faces_diffusion.ipynb)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JfKCrIlDu-E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CIANNA BETA DEV installation**\n",
        "\n",
        "/!\\ WARNING /!\\\n",
        "This beta version is not suited for general application and has been modified for the specific case covered in this notebook. Some function might have a different behavior than the expected one.\n",
        "Do not use outside this notebook !"
      ],
      "metadata": {
        "id": "vIXMFIFmvYzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query GPU allocation and properties\n",
        "\n",
        "If nvidia-smi fail, it might indicate that you launched the colab session whithout GPU reservation.  \n",
        "To change the type of reservation go to \"Runtime\"->\"Change runtime type\" and select \"GPU\" as your hardware accelerator."
      ],
      "metadata": {
        "id": "Ke8s2bCZvk1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi\n",
        "\n",
        "cd /content/\n",
        "\n",
        "git clone https://github.com/NVIDIA/cuda-samples/\n",
        "\n",
        "cd /content/cuda-samples/Samples/1_Utilities/deviceQuery/\n",
        "\n",
        "cmake CMakeLists.txt\n",
        "\n",
        "make SMS=\"50 60 70 80\"\n",
        "\n",
        "./deviceQuery | grep Capability | cut -c50- > ~/cuda_infos.txt\n",
        "./deviceQuery | grep \"CUDA Driver Version / Runtime Version\" | cut -c57- >> ~/cuda_infos.txt\n",
        "\n",
        "cd ~/"
      ],
      "metadata": {
        "id": "AHq06Uwk49Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are granted a GPU that supports high FP16 compute scaling (e.g the Tesla T4), it is advised to change the mixed_precision parameter in the last cell to \"FP16C_FP32A\".  \n",
        "See the detail description on mixed precision support with CIANNA on the [Systeme Requirements](https://github.com/Deyht/CIANNA/wiki/1\\)-System-Requirements) wiki page."
      ],
      "metadata": {
        "id": "tZ-lmHiRBFwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clone CIANNA git repository"
      ],
      "metadata": {
        "id": "A1SJ6-x8vqsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "wget https://share.obspm.fr/s/KNw8aYAEjfxJzsR/download/CIANNA_exp_07_02_25.tar.gz\n",
        "tar -xvzf CIANNA_exp_07_02_25.tar.gz\n",
        "mv CIANNA_exp_07_02_25 CIANNA\n",
        "\n",
        "cd CIANNA"
      ],
      "metadata": {
        "id": "_uptvrov55YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compiling CIANNA for the allocated GPU generation\n",
        "\n",
        "There is no guaranteed forward or backward compatibility between Nvidia GPU generation, and some capabilities are generation specific. For these reasons, CIANNA must be provided the platform GPU generation at compile time.\n",
        "The following cell will automatically update all the necessary files based on the detected GPU, and compile CIANNA."
      ],
      "metadata": {
        "id": "JYGPC3OUv0td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/CIANNA\n",
        "\n",
        "mult=\"10\"\n",
        "cat ~/cuda_infos.txt\n",
        "comp_cap=\"$(sed '1!d' ~/cuda_infos.txt)\"\n",
        "cuda_vers=\"$(sed '2!d' ~/cuda_infos.txt)\"\n",
        "\n",
        "lim=\"11.1\"\n",
        "old_arg=$(awk '{if ($1 < $2) print \"-D CUDA_OLD\";}' <<<\"${cuda_vers} ${lim}\")\n",
        "\n",
        "sm_val=$(awk '{print $1*$2}' <<<\"${mult} ${comp_cap}\")\n",
        "\n",
        "gen_val=$(awk '{if ($1 >= 80) print \"-D GEN_AMPERE\"; else if($1 >= 70) print \"-D GEN_VOLTA\";}' <<<\"${sm_val}\")\n",
        "\n",
        "sed -i \"s/.*arch=sm.*/\\\\t\\tcuda_arg=\\\"\\$cuda_arg -D CUDA -D comp_CUDA -lcublas -lcudart -arch=sm_$sm_val $old_arg $gen_val\\\"/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" src/python_module_setup.py\n",
        "\n",
        "./compile.cp CUDA PY_INTERF\n",
        "\n",
        "mv src/build/lib.linux-x86_64-* src/build/lib.linux-x86_64"
      ],
      "metadata": {
        "id": "HGJUvmWW7YE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing CIANNA installation\n",
        "\n",
        "**IMPORTANT NOTE**   \n",
        "CIANNA is mainly used in a script fashion and was not designed to run in notebooks. Every cell code that directly invokes CIANNA functions must be run as a script to avoid possible errors.  \n",
        "To do so, the cell must have the following structure.\n",
        "\n",
        "```\n",
        "%%shell\n",
        "\n",
        "cd /content/CIANNA\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "[... your python code ...]\n",
        "\n",
        "EOF\n",
        "```\n",
        "\n",
        "This syntax allows one to easily edit python code in the notebook while running the cell as a script. Note that all the notebook variables can not be accessed by the cell in this context.\n"
      ],
      "metadata": {
        "id": "vbnBhbIL8wv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CAT Faces for generative models**\n",
        "\n",
        "This dataset comprises 15747 images of cat faces close-up at a 64x64 resolution. There is no labels as it is intended for training generative models."
      ],
      "metadata": {
        "id": "Lml8Hhi4rZjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Handling"
      ],
      "metadata": {
        "id": "XdOAXvtUwvdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import albumentations as A\n",
        "\n",
        "def cosin_schedule(t, T, s):\n",
        "\treturn np.cos((t/T+s)/(1.0+s)*(np.pi/2))**2\n",
        "\n",
        "im_size = 64\n",
        "im_depth = 3\n",
        "\n",
        "if(not os.path.isdir(\"cats\")):\n",
        "\t\tos.system(\"wget https://share.obspm.fr/s/58BkbEaALbsSWNG/download/cat_faces_gen.tar.gz\")\n",
        "\t\tos.system(\"tar -xzf cat_faces_gen.tar.gz\")\n",
        "\n",
        "\n",
        "file_names = glob.glob(\"cats/*.jpg\")\n",
        "nb_raw_images = 15747\n",
        "\n",
        "n_step = 200\n",
        "\n",
        "min_signal_rate = 0.05**2\n",
        "max_signal_rate = 0.95**2\n",
        "\n",
        "p_alpha_t = cosin_schedule(np.linspace(0,n_step, n_step),n_step,0.0008)\n",
        "p_alpha_t -= np.min(p_alpha_t[:-1])\n",
        "p_alpha_t /= np.max(p_alpha_t[:-1])\n",
        "\n",
        "p_alpha_t = p_alpha_t * (max_signal_rate - min_signal_rate) + min_signal_rate\n",
        "\n",
        "transform = A.Compose([\n",
        "\tA.ColorJitter(brightness=(0.9,1.1), contrast=(0.9,1.1), saturation=(0.9,1.1), hue=0.05, p=1.0),\n",
        "\tA.HorizontalFlip(p=0.5),\n",
        "])\n",
        "\n",
        "def create_batch(nb_size):\n",
        "\n",
        "\tdata = np.zeros((nb_size,im_size*im_size*(im_depth+1)), dtype=\"float32\")\n",
        "\ttargets = np.zeros((nb_size,im_size*im_size*im_depth), dtype=\"float32\")\n",
        "\n",
        "\tfor i in range(0,nb_size):\n",
        "\t\ti_d = int(np.random.random()*nb_raw_images)\n",
        "\n",
        "\t\tpatch = np.asarray(Image.open(file_names[i_d]))\n",
        "\t\ttransformed = transform(image=patch)\n",
        "\t\tpatch = (transformed['image']/255.0)*2.0 - 1.0\n",
        "\n",
        "\t\tstep = np.random.randint(0,n_step-2)\n",
        "\n",
        "\t\tnoise_patch = np.random.normal(loc=0.0, scale=1.0, size=(im_size,im_size,im_depth))\n",
        "\n",
        "\t\tpatch_in = np.sqrt(p_alpha_t[step+1])*patch + np.sqrt(1.0-p_alpha_t[step+1])*noise_patch\n",
        "\n",
        "\t\tfor depth in range(0,im_depth):\n",
        "\t\t\tdata[i,depth*im_size*im_size:(depth+1)*im_size*im_size] = (np.copy(patch_in[:,:,depth]).flatten(\"C\"))\n",
        "\t\t\ttargets[i,depth*im_size*im_size:(depth+1)*im_size*im_size] = noise_patch[:,:,depth].flatten(\"C\")\n",
        "\t\tdata[i,3*im_size*im_size:4*im_size*im_size] = np.sqrt(1.0-p_alpha_t[step+1])\n",
        "\n",
        "\treturn data, targets\n"
      ],
      "metadata": {
        "id": "_FtxcajXtSnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize random subset of raw data"
      ],
      "metadata": {
        "id": "Uhim2jOGBKjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sq_size = 5\n",
        "\n",
        "im_index = np.random.randint(0,nb_raw_images, sq_size**2)\n",
        "\n",
        "fig, axs = plt.subplots(sq_size, sq_size, figsize=(1.4*sq_size,1.4*sq_size), dpi=250, constrained_layout=True)\n",
        "\n",
        "patch = np.zeros((im_size, im_size,3))\n",
        "for i in range(0, sq_size):\n",
        "  for j in range(0, sq_size):\n",
        "    axs[i][j].set_axis_off()\n",
        "    patch = np.asarray(Image.open(file_names[im_index[i*sq_size+j]]))\n",
        "    axs[i][j].imshow(patch)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v8agIFWyBOKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize a training noise chain\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ASy6tp2Rw2p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps_list = np.arange(0,n_step, 20)\n",
        "\n",
        "print (steps_list)\n",
        "\n",
        "im_id = 0\n",
        "\n",
        "patch = np.asarray(Image.open(file_names[im_id]))\n",
        "transformed = transform(image=patch)\n",
        "patch = (transformed['image']/255.0)*2.0 - 1.0\n",
        "\n",
        "fig, axs = plt.subplots(1, len(steps_list), figsize=(2*len(steps_list),2), dpi=250, constrained_layout=True)\n",
        "\n",
        "for i in range(len(steps_list)):\n",
        "  noise_patch = np.random.normal(loc=0.0, scale=1.0, size=(im_size,im_size,im_depth))\n",
        "  patch_in = np.sqrt(p_alpha_t[steps_list[i]])*patch + np.sqrt(1.0-p_alpha_t[steps_list[i]])*noise_patch\n",
        "\n",
        "  axs[i].set_axis_off()\n",
        "  axs[i].imshow(np.clip((patch_in+1.0)*0.5,0.0,1.0))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f3YVQBS5wy_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize a training batch"
      ],
      "metadata": {
        "id": "RBPXdhWc4izD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sq_size = 5\n",
        "\n",
        "data_augm, target_augm = create_batch(sq_size**2)\n",
        "\n",
        "fig, axs = plt.subplots(sq_size, sq_size, figsize=(1.4*sq_size,1.4*sq_size), dpi=250, constrained_layout=True)\n",
        "\n",
        "patch = np.zeros((im_size, im_size,3))\n",
        "for i in range(0, sq_size):\n",
        "  for j in range(0, sq_size):\n",
        "    axs[i][j].set_axis_off()\n",
        "    raw_patch = np.clip((data_augm[i*sq_size+j]+1.0)*0.5,0.0,1.0)\n",
        "    for k in range(0,im_depth):\n",
        "      patch[:,:,k] = np.reshape(raw_patch[k*im_size*im_size:(k+1)*im_size*im_size], (im_size, im_size))\n",
        "    axs[i][j].imshow(patch)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6WqCpGTa4eOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate a batch of image from pre-trained model using probabilistic sampling"
      ],
      "metadata": {
        "id": "sdi9-6X16aNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import os, sys\n",
        "sys.path.insert(0,glob.glob('/content/CIANNA/src/build/lib.*/')[-1])\n",
        "import CIANNA_exp as cnn\n",
        "\n",
        "def i_ar(int_list):\n",
        "\treturn np.array(int_list, dtype=\"int\")\n",
        "\n",
        "def f_ar(float_list):\n",
        "\treturn np.array(float_list, dtype=\"float32\")\n",
        "\n",
        "def cosin_schedule(t, T, s):\n",
        "\treturn np.cos((t/T+s)/(1.0+s)*(np.pi/2))**2\n",
        "\n",
        "im_size = 64\n",
        "im_depth = 3\n",
        "\n",
        "f_im_s = im_size*im_size\n",
        "im_s = f_im_s*im_depth\n",
        "\n",
        "#Was trained with 200, but can be modified\n",
        "n_step = 40\n",
        "\n",
        "min_signal_rate = 0.05**2\n",
        "max_signal_rate = 0.95**2\n",
        "\n",
        "p_alpha_t = cosin_schedule(np.linspace(0,n_step, n_step),n_step,0.0008)\n",
        "p_alpha_t -= np.min(p_alpha_t[:-1])\n",
        "p_alpha_t /= np.max(p_alpha_t[:-1])\n",
        "\n",
        "p_alpha_t = p_alpha_t * (max_signal_rate - min_signal_rate) + min_signal_rate\n",
        "\n",
        "\n",
        "cnn.init(in_dim=i_ar([im_size,im_size]), in_nb_ch=im_depth+1, out_dim=im_s, \\\n",
        "\t\tbias=0.1, b_size=8, use_wema=1, comp_meth=\"C_CUDA\", dynamic_load=1, mixed_precision=\"FP32C_FP32A\") #Change to C_BLAS or C_NAIV\n",
        "\n",
        "if(not os.path.isfile(\"cat_gen_diff_beta_large.dat\")):\n",
        "\t\tos.system(\"wget https://share.obspm.fr/s/QRBHAbt2wprxZNR/download/cat_gen_diff_beta_large_bin.dat\")\n",
        "\n",
        "cnn.load(\"cat_gen_diff_beta_large.dat\",0,0)\n",
        "\n",
        "nb_test = 32\n",
        "n_dif_chain = 5\n",
        "n_dif_steps = 10\n",
        "sq_size = 5\n",
        "\n",
        "#probabilistic or deterministic\n",
        "inference_mode = \"probabilistic\"\n",
        "#Set eta noise counterpart to 0\n",
        "\n",
        "#For testing on static input noise\n",
        "input_images = np.zeros((nb_test,f_im_s*(im_depth+1)), dtype=\"float32\")\n",
        "targets_diff = np.zeros((nb_test,f_im_s*im_depth), dtype=\"float32\")\n",
        "input_noise = np.random.normal(loc=0.0, scale=1.0, size=(nb_test,f_im_s*im_depth))\n",
        "\n",
        "patch = np.zeros((im_size, im_size,im_depth))\n",
        "interp = \"bilinear\"\n",
        "\n",
        "\n",
        "input_images[:,0:im_s] = input_noise[:,:]\n",
        "input_images[:,im_s:] = np.sqrt(1.0-p_alpha_t[-2])\n",
        "\n",
        "cnn.create_dataset(\"TEST\", nb_test, input_images, targets_diff)\n",
        "\n",
        "#For display of several diffusion step of the same input\n",
        "fig1, axs1 = plt.subplots(n_dif_chain, n_dif_steps, figsize=(n_dif_steps*2, n_dif_chain*2), dpi=250, constrained_layout=True)\n",
        "fig2, axs2 = plt.subplots(n_dif_chain, n_dif_steps, figsize=(n_dif_steps*2, n_dif_chain*2), dpi=250, constrained_layout=True)\n",
        "\n",
        "block_size = n_step // n_dif_steps\n",
        "\n",
        "dif_im_count = 0\n",
        "for step in range(1,n_step-1):\n",
        "\tcnn.forward(saving=2, no_error=1, silent=0)\n",
        "\n",
        "\tfile_name = \"fwd_res/net0_%04d.dat\"%(0)\n",
        "\tfwd_dat = np.fromfile(file_name, dtype=\"float32\")\n",
        "\tfwd_dat = np.reshape(fwd_dat,(nb_test,im_s))\n",
        "\n",
        "\tnp1 = n_step-(step+1)\n",
        "\tnp2 = n_step-(step+2)\n",
        "\n",
        "\tnew_images = (input_images[:,0:im_s] - np.sqrt(1.0-p_alpha_t[np1])*fwd_dat[:,:])/(np.sqrt(p_alpha_t[np1]))\n",
        "\tpred_noise = fwd_dat\n",
        "\n",
        "\tif(inference_mode == \"probabilistic\"):\n",
        "\t\teta = np.sqrt((1-p_alpha_t[np2])/(1-p_alpha_t[np1]))*np.sqrt(1-p_alpha_t[np1]/p_alpha_t[np2])\n",
        "\telse:\n",
        "\t\teta = 0.0\n",
        "\n",
        "\tnew_noise = np.random.normal(loc=0.0, scale=1.0, size=(nb_test,im_s))\n",
        "\tinput_images[:,0:im_s] = np.sqrt(p_alpha_t[np2])*new_images[:,:] + np.sqrt(1.0-p_alpha_t[np2] - eta**2)*pred_noise + eta*new_noise[:,:]\n",
        "\tinput_images[:,im_s:] =  np.sqrt(1.0-p_alpha_t[np2])\n",
        "\n",
        "\tj = int(step/block_size)\n",
        "\n",
        "\tif((step+1)%block_size == 0):\n",
        "\t\tfor k in range(0,n_dif_chain):\n",
        "\t\t\tfor depth in range(0,im_depth):\n",
        "\t\t\t\tpatch[:,:,depth] = np.clip(np.reshape(new_images[k][depth*f_im_s:(depth+1)*f_im_s],(im_size,im_size)),-1.0,1.0)\n",
        "\t\t\taxs1[k][j].imshow((patch[:,:,:]+1.0)*0.5, vmax=0.99, vmin=0.01, interpolation=interp)\n",
        "\t\t\taxs1[k][j].axis('off')\n",
        "\t\t\tfor depth in range(0,im_depth):\n",
        "\t\t\t\tpatch[:,:,depth] = np.clip(np.reshape(input_images[k][depth*f_im_s:(depth+1)*f_im_s],(im_size,im_size)),-1.0,1.0)\n",
        "\t\t\taxs2[k][j].imshow((patch[:,:,:]+1.0)*0.5, vmax=0.99, vmin=0.01, interpolation=interp)\n",
        "\t\t\taxs2[k][j].axis('off')\n",
        "\n",
        "\tcnn.delete_dataset(\"TEST\", silent=0)\n",
        "\tcnn.create_dataset(\"TEST\", nb_test, input_images, targets_diff, silent=0)\n",
        "\n",
        "\n",
        "cnn.forward(saving=2, no_error=1, silent=0)\n",
        "cnn.delete_dataset(\"TEST\", silent=0)\n",
        "\n",
        "fwd_dat = np.fromfile(file_name, dtype=\"float32\")\n",
        "fwd_dat = np.reshape(fwd_dat,(nb_test,im_s))\n",
        "\n",
        "new_images = (input_images[:,0:im_s] - np.sqrt(1.0-p_alpha_t[0])*fwd_dat[:,:])/(np.sqrt(p_alpha_t[0]))\n",
        "\n",
        "for k in range(0,n_dif_chain):\n",
        "\tfor depth in range(0,im_depth):\n",
        "\t\tpatch[:,:,depth] = np.clip(np.reshape(new_images[k][depth*f_im_s:(depth+1)*f_im_s],(im_size,im_size)),-1.0,1.0)\n",
        "\taxs1[k][n_dif_steps-1].imshow((patch[:,:,:]+1.0)*0.5, vmax=0.99, vmin=0.01, interpolation=interp)\n",
        "\taxs1[k][n_dif_steps-1].axis('off')\n",
        "\taxs2[k][n_dif_steps-1].imshow((patch[:,:,:]+1.0)*0.5, vmax=0.99, vmin=0.01, interpolation=interp)\n",
        "\taxs2[k][n_dif_steps-1].axis('off')\n",
        "\n",
        "fig1.savefig(\"dif_chain_img_fig.png\", dpi=250)\n",
        "fig2.savefig(\"dif_chain_noisy_img_fig.png\", dpi=250)\n",
        "\n",
        "#For display of mulitple final generations for multiple inputs\n",
        "fig, axs = plt.subplots(sq_size, sq_size, figsize=(1.4*sq_size,1.4*sq_size), dpi=250, constrained_layout=True)\n",
        "\n",
        "fpatch = np.zeros((im_size, im_size,3))\n",
        "for i in range(0, sq_size):\n",
        "\tfor j in range(0, sq_size):\n",
        "\t\taxs[i][j].set_axis_off()\n",
        "\t\traw_patch = np.clip((new_images[i*sq_size+j]+1.0)*0.5,0.0,1.0)\n",
        "\t\tfor k in range(0,im_depth):\n",
        "\t\t\tpatch[:,:,k] = np.reshape(raw_patch[k*f_im_s:(k+1)*f_im_s], (im_size, im_size))\n",
        "\t\taxs[i][j].imshow(patch)\n",
        "plt.savefig(\"all_dif_fig.png\", dpi=250)\n",
        "\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "4ca52h-091tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "im = Image.open(\"dif_chain_img_fig.png\")\n",
        "plt.figure(figsize=(8,8), dpi=250)\n",
        "plt.imshow(im)\n",
        "plt.gca().axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "im = Image.open(\"dif_chain_noisy_img_fig.png\")\n",
        "plt.figure(figsize=(8,8), dpi=250)\n",
        "plt.imshow(im)\n",
        "plt.gca().axis('off')\n",
        "plt.show()\n",
        "\n",
        "im = Image.open(\"all_dif_fig.png\")\n",
        "plt.figure(figsize=(4,4), dpi=250)\n",
        "plt.imshow(im)\n",
        "plt.gca().axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yjAnfV1KGC9V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
