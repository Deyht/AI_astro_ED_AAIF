{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Galaxy Zoo 2 classification notebook**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deyht/AI_astro_ED_AAIF/blob/main/practical_works/CNN/classification/gz2_classification/gz2_classification.ipynb)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JfKCrIlDu-E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CIANNA installation**"
      ],
      "metadata": {
        "id": "vIXMFIFmvYzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query GPU allocation and properties\n",
        "\n",
        "If nvidia-smi fail, it might indicate that you launched the colab session whithout GPU reservation.  \n",
        "To change the type of reservation go to \"Runtime\"->\"Change runtime type\" and select \"GPU\" as your hardware accelerator."
      ],
      "metadata": {
        "id": "Ke8s2bCZvk1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi\n",
        "\n",
        "cd /content/\n",
        "\n",
        "git clone https://github.com/NVIDIA/cuda-samples/\n",
        "\n",
        "cd /content/cuda-samples/Samples/1_Utilities/deviceQuery/\n",
        "\n",
        "cmake CMakeLists.txt\n",
        "\n",
        "make SMS=\"50 60 70 80\"\n",
        "\n",
        "./deviceQuery | grep Capability | cut -c50- > ~/cuda_infos.txt\n",
        "./deviceQuery | grep \"CUDA Driver Version / Runtime Version\" | cut -c57- >> ~/cuda_infos.txt\n",
        "\n",
        "cd ~/"
      ],
      "metadata": {
        "id": "AHq06Uwk49Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are granted a GPU that does not support FP16 computation, it is advised to change the mixed precision method to FP32C_FP32A in the corresponding cells.  \n",
        "See the detail description on mixed precision support with CIANNA on the [Systeme Requirements](https://github.com/Deyht/CIANNA/wiki/1\\)-System-Requirements) wiki page."
      ],
      "metadata": {
        "id": "tZ-lmHiRBFwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clone CIANNA git repository"
      ],
      "metadata": {
        "id": "A1SJ6-x8vqsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "git clone https://github.com/Deyht/CIANNA\n",
        "\n",
        "cd CIANNA"
      ],
      "metadata": {
        "id": "_uptvrov55YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compiling CIANNA for the allocated GPU generation\n",
        "\n",
        "There is no guaranteed forward or backward compatibility between Nvidia GPU generation, and some capabilities are generation specific. For these reasons, CIANNA must be provided the platform GPU generation at compile time.\n",
        "The following cell will automatically update all the necessary files based on the detected GPU, and compile CIANNA."
      ],
      "metadata": {
        "id": "JYGPC3OUv0td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/CIANNA\n",
        "\n",
        "mult=\"10\"\n",
        "cat ~/cuda_infos.txt\n",
        "comp_cap=\"$(sed '1!d' ~/cuda_infos.txt)\"\n",
        "cuda_vers=\"$(sed '2!d' ~/cuda_infos.txt)\"\n",
        "\n",
        "lim=\"11.1\"\n",
        "old_arg=$(awk '{if ($1 < $2) print \"-D CUDA_OLD\";}' <<<\"${cuda_vers} ${lim}\")\n",
        "\n",
        "sm_val=$(awk '{print $1*$2}' <<<\"${mult} ${comp_cap}\")\n",
        "\n",
        "gen_val=$(awk '{if ($1 >= 80) print \"-D GEN_AMPERE\"; else if($1 >= 70) print \"-D GEN_VOLTA\";}' <<<\"${sm_val}\")\n",
        "\n",
        "sed -i \"s/.*arch=sm.*/\\\\t\\tcuda_arg=\\\"\\$cuda_arg -D CUDA -D comp_CUDA -lcublas -lcudart -arch=sm_$sm_val $old_arg $gen_val\\\"/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" src/python_module_setup.py\n",
        "\n",
        "./compile.cp CUDA PY_INTERF\n",
        "\n",
        "mv src/build/lib.linux-x86_64-* src/build/lib.linux-x86_64"
      ],
      "metadata": {
        "id": "HGJUvmWW7YE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CIANNA notebook guideline\n",
        "\n",
        "**IMPORTANT NOTE**   \n",
        "CIANNA is mainly used in a script fashion and was not designed to run in notebooks. Every cell code that directly invokes CIANNA functions must be run as a script to avoid possible errors.  \n",
        "To do so, the cell must have the following structure.\n",
        "\n",
        "```\n",
        "%%shell\n",
        "\n",
        "cd /content/CIANNA\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "[... your python code ...]\n",
        "\n",
        "EOF\n",
        "```\n",
        "\n",
        "This syntax allows one to easily edit python code in the notebook while running the cell as a script. Note that all the notebook variables can not be accessed by the cell in this context.\n"
      ],
      "metadata": {
        "id": "vbnBhbIL8wv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Galaxy Zoo 2 classification**\n",
        "\n",
        "In the original Galaxy Zoo project, volunteers classified images of Sloan Digital Sky Survey galaxies as belonging to one of six categories - elliptical, clockwise spiral, anticlockwise spiral, edge-on , star/don't know, or merger. GZ2 extends the original Galaxy Zoo classifications for a subsample of the brightest and largest galaxies in the Legacy release, measuring more detailed morphological features. This includes galactic bars, spiral arm and pitch angle, bulges, edge-on galaxies, relative ellipticities, and many others.\n",
        "\n",
        "There are 243,434 images in total, all resized to a 424x424 resolution. Images are composed so the main object is centered and a part of the environment is visible. This implies that the FoV of each image is different.\n",
        "For simplicity, we will use cropped and resized images that are more zoomed in toward the object and resized to a either a 64x64 or a 128x128 image resolution.\n",
        "\n",
        "Details on the classification process can be found in [Hart et al. 2016](https://academic.oup.com/mnras/article/461/4/3663/2608720?login=true)\n"
      ],
      "metadata": {
        "id": "Lml8Hhi4rZjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading and visualizing the data"
      ],
      "metadata": {
        "id": "XdOAXvtUwvdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/helper.py\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal, ndimage\n",
        "from PIL import Image\n",
        "from PIL import ImageFilter\n",
        "\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import os,sys\n",
        "import gc\n",
        "\n",
        "\n",
        "image_size = 128\n",
        "im_depth = 3\n",
        "\n",
        "\n",
        "if(not os.path.isdir(\"gz2_images_%dx%d\"%(image_size,image_size))):\n",
        "\tprint(\"Downloading dataset ...\")\n",
        "\tif(image_size == 64):\n",
        "\t\tos.system(\"wget https://share.obspm.fr/s/HJAXeYkiBsK4P8F/download/gz2_images_64x64.tar.gz\")\n",
        "\t\tos.system(\"tar -xzf gz2_images_64x64.tar.gz\")\n",
        "\telif(image_size == 128):\n",
        "\t\tos.system(\"wget https://share.obspm.fr/s/fGRRQX6zao43mbE/download/gz2_images_128x128.tar.gz\")\n",
        "\t\tos.system(\"tar -xzf gz2_images_128x128.tar.gz\")\n",
        "\telse:\n",
        "\t\tprint(\"Invalid image size ...\")\n",
        "\t\texit()\n",
        "\n",
        "catalog_path = \"gz2_filename_mapping_with_class.csv\"\n",
        "\n",
        "if(not os.path.isfile(catalog_path)):\n",
        "\t\tos.system(\"wget https://share.obspm.fr/s/fKqtaAQTQAt7k7J/download/gz2_filename_mapping_with_class.csv\")\n",
        "\n",
        "\n",
        "gz2_catalog_header = np.genfromtxt(catalog_path, delimiter=\",\", max_rows=1, dtype=\"str\")\n",
        "\n",
        "gz2_catalog = np.genfromtxt(catalog_path, delimiter=\",\", skip_header=1, dtype=\"str\")\n",
        "\n",
        "np.random.shuffle(gz2_catalog)\n",
        "\n",
        "do_not_exist = np.zeros(np.shape(gz2_catalog), dtype=\"int\")\n",
        "\n",
        "for i in range(0, np.shape(gz2_catalog)[0]):\n",
        "\tpath = \"gz2_images_%dx%d/%s.jpg\"%(image_size, image_size, gz2_catalog[i,1])\n",
        "\tif(not os.path.isfile(path)):\n",
        "\t\tdo_not_exist[i] = 1\n",
        "\n",
        "#Remove around 100 objects for which the image is not provided\n",
        "index = np.where(do_not_exist == 1)[0]\n",
        "\n",
        "gz2_catalog = np.delete(gz2_catalog, index, axis=0)\n",
        "\n",
        "del(do_not_exist)\n",
        "gc.collect()\n",
        "\n",
        "total_nb_images = np.shape(gz2_catalog)[0]\n",
        "\n",
        "class_list = [\"A\", \"Ec\", \"Ei\", \"Er\", \"SBa\", \"SBb\", \"SBc\", \"SBd\", \"Sa\", \"Sb\", \"Sc\", \"Sd\", \"Se\"]\n",
        "class_list = [\"E\", \"SB\", \"S\"]\n",
        "\n",
        "frac_test = 0.05\n",
        "\n",
        "nb_class = int(len(class_list))\n",
        "class_count_train = np.zeros(nb_class, dtype=\"int\")\n",
        "class_count_test = np.zeros(nb_class, dtype=\"int\")\n",
        "\n",
        "for i in range(0, nb_class):\n",
        "\tindex = np.where(gz2_catalog[:,2].astype(\"<U%d\"%(len(class_list[i]))) == class_list[i])[0]\n",
        "\tclass_count_test[i] = int(frac_test*np.shape(index)[0])\n",
        "\tclass_count_train[i] = np.shape(index)[0] - class_count_test[i]\n",
        "\n",
        "total_nb_train = np.sum(class_count_train)\n",
        "total_nb_test = np.sum(class_count_test)\n",
        "\n",
        "filename_to_class_array_train = np.zeros((total_nb_train,3), dtype=\"int\")\n",
        "filename_to_class_array_test = np.zeros((total_nb_test,3), dtype=\"int\")\n",
        "\n",
        "train_cumsum = np.zeros(nb_class+1, dtype=\"int\")\n",
        "test_cumsum = np.zeros(nb_class+1, dtype=\"int\")\n",
        "\n",
        "train_cumsum[1:] = np.cumsum(class_count_train)\n",
        "test_cumsum[1:] = np.cumsum(class_count_test)\n",
        "\n",
        "for i in range(0, nb_class):\n",
        "\tindex = np.where(gz2_catalog[:,2].astype(\"<U%d\"%(len(class_list[i]))) == class_list[i])[0]\n",
        "\n",
        "\tfilename_to_class_array_train[train_cumsum[i]:train_cumsum[i+1], 0] = gz2_catalog[index[:class_count_train[i]],1]\n",
        "\tfilename_to_class_array_train[train_cumsum[i]:train_cumsum[i+1], 1] = i\n",
        "\tfilename_to_class_array_test[test_cumsum[i]:test_cumsum[i+1], 0] = gz2_catalog[index[class_count_train[i]:],1]\n",
        "\tfilename_to_class_array_test[test_cumsum[i]:test_cumsum[i+1], 1] = i\n",
        "\n",
        "transform = A.Compose([\n",
        "\tA.HorizontalFlip(p=0.5),\n",
        "\tA.VerticalFlip(p=0.5),\n",
        "])\n",
        "\n",
        "training_rebalance = np.clip(class_count_train[:],0,4000).astype(\"float\")\n",
        "print(training_rebalance)\n",
        "training_rebalance /= np.sum(training_rebalance)\n",
        "\n",
        "nb_im_train = 2048\n",
        "nb_im_test = total_nb_test\n",
        "\n",
        "input_data = np.zeros((nb_im_train,image_size*image_size*(im_depth+1)), dtype=\"float32\")\n",
        "targets = np.zeros((nb_im_train,nb_class), dtype=\"float32\")\n",
        "\n",
        "input_test = np.zeros((nb_im_test,image_size*image_size*(im_depth+1)), dtype=\"float32\")\n",
        "targets_test = np.zeros((nb_im_test,nb_class), dtype=\"float32\")\n",
        "\n",
        "zero_target = np.zeros(nb_class)\n",
        "\n",
        "def create_train_batch(visual=0):\n",
        "\n",
        "\tif(visual):\n",
        "\t\tfig, axs = plt.subplots(4,5, figsize=(5,4), dpi=250, constrained_layout=True)\n",
        "\n",
        "\tfor i in range(0,nb_im_train):\n",
        "\t\tr_class = np.random.choice(np.arange(0,nb_class), p=training_rebalance)\n",
        "\n",
        "\t\ti_d = train_cumsum[r_class] + int(np.random.random()*class_count_train[r_class])\n",
        "\t\tpath = \"gz2_images_%dx%d/%d.jpg\"%(image_size,image_size,filename_to_class_array_train[i_d,0])\n",
        "\n",
        "\t\tpatch = np.asarray(Image.open(path))\n",
        "\t\ttransformed = transform(image=patch)\n",
        "\t\tpatch = (transformed['image']/255.0)\n",
        "\n",
        "\t\tfor depth in range(0,im_depth):\n",
        "\t\t\tinput_data[i,depth*image_size*image_size:(depth+1)*image_size*image_size] = (np.copy(patch[:,:,depth]).flatten(\"C\"))\n",
        "\t\tinput_data[i,3*image_size*image_size:] = 0.0\n",
        "\t\ttargets[i,:] = 0\n",
        "\t\ttargets[i,filename_to_class_array_train[i_d,1]] = 1\n",
        "\n",
        "\t\tif(visual and i < 20):\n",
        "\t\t\taxs[i//5][i%5].imshow(patch)\n",
        "\t\t\taxs[i//5][i%5].set_axis_off()\n",
        "\t\t\taxs[i//5][i%5].text(0.1,0.1, \"%s\"%(class_list[filename_to_class_array_train[i_d,1]]), c=\"limegreen\", va=\"top\", fontsize=6)\n",
        "\n",
        "\tif(visual):\n",
        "\t\tplt.savefig(\"training_set_example.jpg\", dpi=250)\n",
        "\t\treturn\n",
        "\n",
        "\treturn input_data, targets\n",
        "\n",
        "\n",
        "def create_test_batch():\n",
        "\n",
        "\tfor i in range(0,nb_im_test):\n",
        "\t\tpath = \"gz2_images_%dx%d/%d.jpg\"%(image_size, image_size, filename_to_class_array_test[i,0])\n",
        "\n",
        "\t\tpatch = np.asarray(Image.open(path))/255.0\n",
        "\n",
        "\t\tfor depth in range(0,im_depth):\n",
        "\t\t\tinput_test[i,depth*image_size*image_size:(depth+1)*image_size*image_size] = (np.copy(patch[:,:,depth]).flatten(\"C\"))\n",
        "\t\tinput_test[i,3*image_size*image_size:] = 0.0\n",
        "\t\ttargets_test[i,:] = 0\n",
        "\t\ttargets_test[i,filename_to_class_array_test[i,1]] = 1\n",
        "\n",
        "\treturn input_test, targets_test\n"
      ],
      "metadata": {
        "id": "Fqo2Tgd4O0uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "#Will download the dataset at the fist call\n",
        "from helper import *\n",
        "\n",
        "create_train_batch(visual=1)\n",
        "\n",
        "EOF"
      ],
      "metadata": {
        "id": "_FtxcajXtSnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AI_astro_ED_AAIF/codes/CNN/classification/gz2_classification/\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "im = Image.open(\"training_set_example.jpg\")\n",
        "plt.figure(figsize=(5,4), dpi=200)\n",
        "plt.imshow(im)\n",
        "plt.gca().axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v8agIFWyBOKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the classifier"
      ],
      "metadata": {
        "id": "dFJsfKA3y6wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/AI_astro_ED_AAIF/codes/CNN/classification/gz2_classification/\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "\n",
        "import time\n",
        "import locale\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from threading import Thread\n",
        "\n",
        "from helper import *\n",
        "import numpy as np\n",
        "\n",
        "#Comment to access system wide install\n",
        "import sys, glob\n",
        "sys.path.insert(0,glob.glob('/content/CIANNA/src/build/lib.*/')[-1])\n",
        "import CIANNA as cnn\n",
        "\n",
        "\n",
        "def i_ar(int_list):\n",
        "\treturn np.array(int_list, dtype=\"int\")\n",
        "\n",
        "def f_ar(float_list):\n",
        "\treturn np.array(float_list, dtype=\"float32\")\n",
        "\n",
        "def data_augm():\n",
        "\n",
        "\tdata_augm, targets_augm = create_train_batch()\n",
        "\tcnn.delete_dataset(\"TRAIN_buf\", silent=1)\n",
        "\tcnn.create_dataset(\"TRAIN_buf\", nb_im_train, data_augm, targets_augm, silent=1)\n",
        "\treturn\n",
        "\n",
        "\n",
        "data_train, target_train = create_train_batch()\n",
        "data_valid, target_valid = create_test_batch()\n",
        "data_test, target_test = create_test_batch()\n",
        "\n",
        "cnn.init(in_dim=i_ar([image_size,image_size]), in_nb_ch=im_depth+1, out_dim=nb_class, \\\n",
        "\t\tbias=0.1, b_size=8, comp_meth=\"C_CUDA\", dynamic_load=1, mixed_precision=\"FP16C_FP32A\")\n",
        "\n",
        "cnn.create_dataset(\"TRAIN\", size=nb_im_train, input=data_train, target=target_train)\n",
        "cnn.create_dataset(\"VALID\", size=nb_im_test , input=data_valid, target=target_valid)\n",
        "\n",
        "del(data_valid)\n",
        "\n",
        "load_epoch = 0\n",
        "if (len(sys.argv) > 1):\n",
        "\tload_epoch = int(sys.argv[1])\n",
        "if(load_epoch > 0):\n",
        "\tcnn.load(\"net_save/net0_s%04d.dat\"%load_epoch,load_epoch,0)\n",
        "else:\n",
        "\n",
        "\tcnn.conv(f_size=i_ar([5,5]), nb_filters=16  , padding=i_ar([2,2]), activation=\"RELU\")\n",
        "\tcnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "\tcnn.norm(group_size=4, activation=\"LIN\")\n",
        "\n",
        "\tcnn.conv(f_size=i_ar([3,3]), nb_filters=32  , padding=i_ar([2,2]), activation=\"RELU\")\n",
        "\tcnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "\tcnn.norm(group_size=4, activation=\"LIN\")\n",
        "\n",
        "\tcnn.conv(f_size=i_ar([3,3]), nb_filters=64\t, padding=i_ar([1,1]), activation=\"RELU\")\n",
        "\tcnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "\tcnn.norm(group_size=8, activation=\"LIN\")\n",
        "\n",
        "\tcnn.conv(f_size=i_ar([3,3]), nb_filters=128 , padding=i_ar([1,1]), activation=\"RELU\")\n",
        "\tcnn.conv(f_size=i_ar([1,1]), nb_filters=64  , padding=i_ar([0,0]), activation=\"RELU\")\n",
        "\tcnn.conv(f_size=i_ar([3,3]), nb_filters=128 , padding=i_ar([1,1]), activation=\"RELU\")\n",
        "\tcnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "\tcnn.norm(group_size=16, activation=\"LIN\")\n",
        "\n",
        "\tif(image_size == 128):\n",
        "\t\tcnn.conv(f_size=i_ar([3,3]), nb_filters=128 , padding=i_ar([1,1]), activation=\"RELU\")\n",
        "\t\tcnn.conv(f_size=i_ar([1,1]), nb_filters=64  , padding=i_ar([0,0]), activation=\"RELU\")\n",
        "\t\tcnn.conv(f_size=i_ar([3,3]), nb_filters=128 , padding=i_ar([1,1]), activation=\"RELU\")\n",
        "\t\tcnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "\t\tcnn.norm(group_size=16, activation=\"LIN\")\n",
        "\n",
        "\tcnn.conv(f_size=i_ar([1,1]), nb_filters=nb_class , padding=i_ar([0,0]), activation=\"LIN\")\n",
        "\tcnn.pool(p_size=i_ar([1,1]), p_type=\"AVG\", p_global=1, activation=\"SMAX\")\n",
        "\n",
        "\n",
        "for i in range(load_epoch,1200):\n",
        "\tt = Thread(target=data_augm)\n",
        "\tt.start()\n",
        "\n",
        "\tcnn.train(nb_iter=1, learning_rate=0.002, end_learning_rate=0.0001, lr_decay=0.0012, momentum=0.9, weight_decay=0.001,\n",
        "\t\tconfmat=1, control_interv=10, save_every=50, TC_scale_factor=1.0)\n",
        "\n",
        "\tt.join()\n",
        "\tcnn.swap_data_buffers(\"TRAIN\")\n",
        "\n",
        "\n",
        "EOF"
      ],
      "metadata": {
        "id": "YDDDvKO8y-Em"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}