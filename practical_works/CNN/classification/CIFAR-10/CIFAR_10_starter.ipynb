{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CIFAR-10 notebook**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deyht/AI_astro_ED_AAIF/blob/main/practical_works/CNN/classification/CIFAR-10/CIFAR-10_starter.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Link to the CIANNA github repository**\n",
        "https://github.com/Deyht/CIANNA"
      ],
      "metadata": {
        "id": "JfKCrIlDu-E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CIANNA installation**"
      ],
      "metadata": {
        "id": "vIXMFIFmvYzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query GPU allocation and properties\n",
        "\n",
        "If nvidia-smi fail, it might indicate that you launched the colab session whithout GPU reservation.  \n",
        "To change the type of reservation go to \"Runtime\"->\"Change runtime type\" and select \"GPU\" as your hardware accelerator."
      ],
      "metadata": {
        "id": "Ke8s2bCZvk1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi\n",
        "\n",
        "cd /content/\n",
        "\n",
        "git clone https://github.com/NVIDIA/cuda-samples/\n",
        "\n",
        "cd /content/cuda-samples/Samples/1_Utilities/deviceQuery/\n",
        "\n",
        "cmake CMakeLists.txt\n",
        "\n",
        "make SMS=\"50 60 70 80\"\n",
        "\n",
        "./deviceQuery | grep Capability | cut -c50- > ~/cuda_infos.txt\n",
        "./deviceQuery | grep \"CUDA Driver Version / Runtime Version\" | cut -c57- >> ~/cuda_infos.txt\n",
        "\n",
        "cd ~/"
      ],
      "metadata": {
        "id": "AHq06Uwk49Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are granted a GPU that supports high FP16 compute scaling (e.g the Tesla T4), it is advised to change the mixed_precision parameter in the prediction to \"FP16C_FP32A\".  \n",
        "See the detail description on mixed precision support with CIANNA on the [Systeme Requirements](https://github.com/Deyht/CIANNA/wiki/1\\)-System-Requirements) wiki page."
      ],
      "metadata": {
        "id": "tZ-lmHiRBFwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clone CIANNA git repository"
      ],
      "metadata": {
        "id": "A1SJ6-x8vqsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "git clone https://github.com/Deyht/CIANNA\n",
        "\n",
        "cd CIANNA"
      ],
      "metadata": {
        "id": "_uptvrov55YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compiling CIANNA for the allocated GPU generation\n",
        "\n",
        "There is no guaranteed forward or backward compatibility between Nvidia GPU generation, and some capabilities are generation specific. For these reasons, CIANNA must be provided the platform GPU generation at compile time.\n",
        "The following cell will automatically update all the necessary files based on the detected GPU, and compile CIANNA."
      ],
      "metadata": {
        "id": "JYGPC3OUv0td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/CIANNA\n",
        "\n",
        "mult=\"10\"\n",
        "cat ~/cuda_infos.txt\n",
        "comp_cap=\"$(sed '1!d' ~/cuda_infos.txt)\"\n",
        "cuda_vers=\"$(sed '2!d' ~/cuda_infos.txt)\"\n",
        "\n",
        "lim=\"11.1\"\n",
        "old_arg=$(awk '{if ($1 < $2) print \"-D CUDA_OLD\";}' <<<\"${cuda_vers} ${lim}\")\n",
        "\n",
        "sm_val=$(awk '{print $1*$2}' <<<\"${mult} ${comp_cap}\")\n",
        "\n",
        "gen_val=$(awk '{if ($1 >= 80) print \"-D GEN_AMPERE\"; else if($1 >= 70) print \"-D GEN_VOLTA\";}' <<<\"${sm_val}\")\n",
        "\n",
        "sed -i \"s/.*arch=sm.*/\\\\t\\tcuda_arg=\\\"\\$cuda_arg -D CUDA -D comp_CUDA -lcublas -lcudart -arch=sm_$sm_val $old_arg $gen_val\\\"/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" src/python_module_setup.py\n",
        "\n",
        "./compile.cp CUDA PY_INTERF\n",
        "\n",
        "mv src/build/lib.linux-x86_64-* src/build/lib.linux-x86_64"
      ],
      "metadata": {
        "id": "HGJUvmWW7YE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing CIANNA installation\n",
        "\n",
        "**IMPORTANT NOTE**   \n",
        "CIANNA is mainly used in a script fashion and was not designed to run in notebooks. Every cell code that directly invokes CIANNA functions must be run as a script to avoid possible errors.  \n",
        "To do so, the cell must have the following structure.\n",
        "\n",
        "```\n",
        "%%shell\n",
        "\n",
        "cd /content/CIANNA\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "[... your python code ...]\n",
        "\n",
        "EOF\n",
        "```\n",
        "\n",
        "This syntax allows one to easily edit python code in the notebook while running the cell as a script. Note that all the notebook variables can not be accessed by the cell in this context.\n"
      ],
      "metadata": {
        "id": "vbnBhbIL8wv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CIFAR-10**\n",
        "\n",
        "CIFAR-10 is a lightweight dataset, which comprises 60000 images of 32x32 pixels labeled into 10 classes. 50000 images are used to train supervised learning models, with 5000 examples for each class, and 10000 images are used for testing trained models, with 1000 examples for each class."
      ],
      "metadata": {
        "id": "gd2waB3JYNkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading and visualizing the data\n",
        "\n",
        "\n",
        "We start by downloading and visualizing the raw data."
      ],
      "metadata": {
        "id": "kULtlVy8Y5UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content\n",
        "\n",
        "wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "tar -xzf cifar-10-python.tar.gz"
      ],
      "metadata": {
        "id": "mjcrByRgYof3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "import os, glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "image_size = 32\n",
        "nb_class = 10\n",
        "\n",
        "v_width = 8; v_height = 5\n",
        "nb_images = v_width*v_height\n",
        "\n",
        "f_im_s = image_size*image_size*3\n",
        "\n",
        "batches_meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "class_list = batches_meta[b\"label_names\"]\n",
        "\n",
        "dict_batch = unpickle(glob.glob(\"cifar-10-batches-py/data_batch_*\")[0])\n",
        "data = dict_batch[b\"data\"]\n",
        "labels = dict_batch[b\"labels\"]\n",
        "\n",
        "fig, ax = plt.subplots(v_height, v_width, figsize=(v_width*1.5,v_height*1.5), dpi=200, constrained_layout=True)\n",
        "\n",
        "for i in range(0, v_width*v_height):\n",
        "  c_x = i // v_width; c_y = i % v_width\n",
        "  img_flat = data[i,:]\n",
        "  patch = np.zeros((image_size, image_size, 3), dtype=\"uint8\")\n",
        "  for depth in range(0,3):\n",
        "    patch[:,:,depth] = np.reshape(img_flat[depth*image_size*image_size:(depth+1)*image_size*image_size], (image_size, image_size))\n",
        "\n",
        "  ax[c_x,c_y].imshow(patch, interpolation=\"lanczos\")\n",
        "  ax[c_x,c_y].text(0,0, class_list[labels[i]], c=\"red\", va=\"top\")\n",
        "  ax[c_x,c_y].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-PsyIg0sZmSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data handling and augmentation\n",
        "\n",
        "To ease data manipulation and hyperparameter exploration, we first provide a set of helper functions. To make them accessible within the CIANNA script cells, we need to export them to a Python file. Every time you want to change the content of these functions, you will need to rerun the cell to generate a new .py file. If loaded in an interactive cell, you will need to restart the kernel after changing this file to re-import it properly."
      ],
      "metadata": {
        "id": "OWIupxetzyVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, sys, gc, glob, time, cv2\n",
        "from threading import Thread\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "import pickle\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "def data_prep(nb_images_per_iter, image_size, test_mode=0):\n",
        "  #Data arrays are declared as global so we can work in place to reduce RAM footpring\n",
        "  global raw_train_images, train_classes, g_image_size, raw_test_images, test_classes, input_data, targets, input_val, targets_val\n",
        "\n",
        "  raw_train_images = np.zeros((50000,image_size*image_size*3), dtype=\"uint8\")\n",
        "  train_classes = np.zeros((50000))\n",
        "\n",
        "  raw_test_images = np.zeros((10000,image_size*image_size*3), dtype=\"uint8\")\n",
        "  test_classes = np.zeros((10000))\n",
        "\n",
        "  i = 0\n",
        "  for batch in glob.glob(\"cifar-10-batches-py/data_batch_*\"):\n",
        "    dict_batch = unpickle(batch)\n",
        "    raw_train_images[i*10000:(i+1)*10000,:] = dict_batch[b\"data\"]\n",
        "    train_classes[i*10000:(i+1)*10000] = dict_batch[b\"labels\"]\n",
        "    i += 1\n",
        "\n",
        "  dict_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "  raw_test_images[:,:] = dict_batch[b\"data\"]\n",
        "  test_classes[:] = dict_batch[b\"labels\"]\n",
        "\n",
        "  if(test_mode == 0):\n",
        "    input_data = np.zeros((nb_images_per_iter,3*image_size**2), dtype=\"float32\") #CIANNA expects \"float32\" arrays\n",
        "    targets = np.zeros((nb_images_per_iter,10), dtype=\"float32\")\n",
        "\n",
        "  input_val = np.zeros((10000,3*image_size**2), dtype=\"float32\")\n",
        "  targets_val = np.zeros((10000,10), dtype=\"float32\")\n",
        "\n",
        "\n",
        "def create_augmented_batch(A_transform):\n",
        "\n",
        "  nb_images = np.shape(input_data)[0]\n",
        "  image_size = int(np.sqrt(np.shape(input_data)[1]/3))\n",
        "  patch = np.zeros((image_size, image_size, 3), dtype=\"uint8\")\n",
        "\n",
        "  for i in range(0,nb_images):\n",
        "\n",
        "    l_id = np.random.randint(0,50000)\n",
        "    flat_patch = raw_train_images[l_id]\n",
        "    for depth in range(0,3):\n",
        "      patch[:,:,depth] = np.reshape(flat_patch[depth*image_size*image_size:(depth+1)*image_size*image_size], (image_size, image_size))\n",
        "\n",
        "    transformed = A_transform(image=patch)\n",
        "    patch_aug = transformed['image']\n",
        "\n",
        "    #CIANNA expects data formated as 2D numpy arrays representing a list of flattened images (with every channel flattened after the others)\n",
        "    for depth in range(0,3): #We normalize based on mean pixel value\n",
        "      input_data[i,depth*image_size**2:(depth+1)*image_size**2] = (patch_aug[:,:,depth].flatten(\"C\") - 100.0)/155.0\n",
        "\n",
        "    targets[i,:] = 0.0\n",
        "    targets[i,int(train_classes[l_id])] = 1.0\n",
        "\n",
        "  return input_data, targets\n",
        "\n",
        "\n",
        "def create_validation_set(A_transform):\n",
        "\n",
        "  for i in range(0,10000):\n",
        "\n",
        "    input_val[i,:] = (raw_test_images[i] - 100.0)/155.0\n",
        "\n",
        "    targets_val[i,:] = 0.0\n",
        "    targets_val[i,int(test_classes[i])] = 1.0\n",
        "\n",
        "  return input_val, targets_val\n",
        "\n",
        "\n",
        "def free_data_helper():\n",
        "  global raw_train_images, train_classes, raw_test_images, test_classes, input_data, targets, input_val, targets_val\n",
        "  del (raw_train_images, train_classes, raw_test_images, test_classes, input_data, targets, input_val, targets_val)\n",
        "  return"
      ],
      "metadata": {
        "id": "HOKdPWXQLziA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test the helper functions with a simple example and display the produced images. The next cell illustrates how we can create an augmented batch of images for training from the raw image dataset.  \n",
        "\n",
        "Use this example to test the effect of combining different transform operations for data augmentation. You can also test the impact of the image resolution and of the position of the resize transformation in the augmentation list."
      ],
      "metadata": {
        "id": "bPo_XyxkXTy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "from helper import *\n",
        "\n",
        "batches_meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "class_list = batches_meta[b\"label_names\"]\n",
        "\n",
        "image_size = 32\n",
        "\n",
        "v_width = 8; v_height = 5\n",
        "nb_images_per_iter = v_width*v_height\n",
        "\n",
        "#See Albumentation documentation for a list of existing augmentations\n",
        "train_transform = A.Compose([\n",
        "  #Image resize can be done after all other transform to preserve as much details as possible\n",
        "  #or as the fist operation so other transforms are faster\n",
        "  #A.Resize(image_size,image_size, interpolation=2, p=1.0),\n",
        "  A.HorizontalFlip(p=0.5),\n",
        "  #Affine here act more as an aspect ratio transform than a scaling variation\n",
        "  #A.Affine(..., p=1.0),\n",
        "  #A.ToGray(p=0.02),\n",
        "  #A.ColorJitter(..., p=1.0),\n",
        "  ])\n",
        "\n",
        "val_transform = A.Compose([ #Here only a resize, but val transform could be more complex (center crop, padding, etc)\n",
        "  A.Resize(image_size, image_size, interpolation=2, p=1.0)])\n",
        "\n",
        "data_prep(nb_images_per_iter, image_size)\n",
        "input_data, targets = create_augmented_batch(train_transform)\n",
        "\n",
        "fig, ax = plt.subplots(v_height, v_width, figsize=(v_width*1.5,v_height*1.5), dpi=200, constrained_layout=True)\n",
        "patch = np.zeros((image_size,image_size,3), dtype=\"uint8\")\n",
        "\n",
        "for i in range(0, v_width*v_height):\n",
        "  c_x = i // v_width; c_y = i % v_width\n",
        "  #Images in the augmented input_data array are directly in the CIANNA format.\n",
        "  #We need to convert them back to classical RGB for display.\n",
        "  for depth in range(0,3):\n",
        "    patch[:,:,depth] = np.reshape(input_data[i,depth*image_size**2:(depth+1)*image_size**2]*155 + 100,(image_size,image_size))\n",
        "\n",
        "  ax[c_x,c_y].imshow(patch)\n",
        "  ax[c_x,c_y].text(0, 0, class_list[(np.argmax(targets[i]))], c=\"red\", fontsize=10, clip_on=True, va=\"top\")\n",
        "  ax[c_x,c_y].axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "free_data_helper()"
      ],
      "metadata": {
        "id": "yZYLSyJWXQ8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training a network\n",
        "\n",
        "The following cell allows to train a neural network architecture with CIANNA using dynamical augmentation through Albumentation. The architecture has been left empty as an exercice. Try to implement a woking architecture on this dataset and then try to achieve the highest possible accuracy.\n",
        "\n",
        "*Link to the [CIANNA](https://github.com/Deyht/CIANNA) repository. You can refer to CIANNA's [WIKI page](https://github.com/Deyht/CIANNA/wiki) for a complete framework description. You can also look at the full [API documentation](https://github.com/Deyht/CIANNA/wiki/4\\)-Interface-API-documentation) to add layer types that are absent from the LeNET-5 example.\n",
        "The saved models are available in the \"net_save\" repository that is automatically created when starting network training. The default naming scheme only refers to the training iteration, so rename your saving files with comprehensive information about your model to keep track of your progress. A saved model can be uploaded to a new Colab session for inference or further training.*"
      ],
      "metadata": {
        "id": "6ZeV1bvjRS4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd /content/\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "from helper import *\n",
        "\n",
        "sys.path.insert(0,glob.glob('/content/CIANNA/src/build/lib.*/')[-1])\n",
        "import CIANNA as cnn\n",
        "\n",
        "def i_ar(int_list):\n",
        "  return np.array(int_list, dtype=\"int\")\n",
        "\n",
        "image_size = 32\n",
        "nb_images_per_iter = 4096 #Must likely be reduced if the image size is aumgented so examples can fit in RAM\n",
        "\n",
        "\n",
        "#See Albumentation documentation for a list of existing augmentations\n",
        "train_transform = A.Compose([\n",
        "  #Image resize can be done after all other transform to preserve as much details as possible\n",
        "  #or as the fist operation so other transforms are faster\n",
        "  #A.Resize(image_size,image_size, interpolation=2, p=1.0),\n",
        "  A.HorizontalFlip(p=0.5),\n",
        "  #Affine here act more as an aspect ratio transform than a scaling variation\n",
        "  #A.Affine(..., p=1.0),\n",
        "  #A.ToGray(p=0.02),\n",
        "  #A.ColorJitter(..., p=1.0),\n",
        "  ])\n",
        "\n",
        "val_transform = A.Compose([]) #Images are alreay in the proper format, but create_validation_set expect an augm argument\n",
        "\n",
        "\n",
        "#This funtion allow to launch data augmentation on a separate thread.\n",
        "#This way we can train on the GPU and generate new agumented examples in parallel.\n",
        "def data_augm():\n",
        "  input_data, targets = create_augmented_batch(train_transform)\n",
        "  cnn.delete_dataset(\"TRAIN_buf\", silent=1)\n",
        "  cnn.create_dataset(\"TRAIN_buf\", nb_images_per_iter, input_data[:,:], targets[:,:], silent=1)\n",
        "  return\n",
        "\n",
        "#In case the creation of new augmented data is too long compared to training, you can\n",
        "#increase the number of training iteration over a single augmentation\n",
        "nb_iter_per_augm = 1\n",
        "if(nb_iter_per_augm > 1):\n",
        "  shuffle_frequency = 1\n",
        "else:\n",
        "  shuffle_frequency = 0\n",
        "\n",
        "\n",
        "total_iter = 400 #Should be increased with the complexity of the network and task\n",
        "load_iter = 0 #Used to reload a model at a given iteration\n",
        "\n",
        "if (len(sys.argv) > 1):\n",
        "  load_iter = int(sys.argv[1])\n",
        "\n",
        "start_iter = int(load_iter / nb_iter_per_augm)\n",
        "\n",
        "cnn.init(in_dim=i_ar([image_size,image_size]), in_nb_ch=3, out_dim=10,\n",
        "  bias=0.1, b_size=16, comp_meth='C_CUDA', dynamic_load=1,\n",
        "  mixed_precision=\"FP32C_FP32A\", adv_size=30)\n",
        "\n",
        "data_prep(nb_images_per_iter, image_size)\n",
        "\n",
        "input_val, targets_val = create_validation_set(val_transform)\n",
        "cnn.create_dataset(\"VALID\", 10000, input_val[:,:], targets_val[:,:])\n",
        "cnn.create_dataset(\"TEST\", 10000, input_val[:,:], targets_val[:,:])\n",
        "del (input_val, targets_val) #The python arrays are no longer required after import in CIANNA\n",
        "gc.collect()\n",
        "\n",
        "#Create fist augmentation before parallelization\n",
        "input_data, targets = create_augmented_batch(train_transform)\n",
        "cnn.create_dataset(\"TRAIN\", nb_images_per_iter, input_data[:,:], targets[:,:])\n",
        "\n",
        "if(load_iter > 0):\n",
        "  cnn.load(\"net_save/net0_s%04d.dat\"%load_iter, load_iter, bin=1)\n",
        "else:\n",
        "\n",
        "  ##### -> Declare your network backbone architecture here\n",
        "\n",
        "\n",
        "cnn.print_arch_tex(\"./arch/\", \"arch\", activation=1, dropout=1)\n",
        "\n",
        "for run_iter in range(start_iter,int(total_iter/nb_iter_per_augm)):\n",
        "\n",
        "  t = Thread(target=data_augm)\n",
        "  t.start()\n",
        "\n",
        "  cnn.train(nb_iter=nb_iter_per_augm, learning_rate=0.002, shuffle_every=shuffle_frequency ,\\\n",
        "        control_interv=20, confmat=1, save_every=20, silent=0, save_bin=1, TC_scale_factor=128.0)\n",
        "\n",
        "  if(run_iter == start_iter):\n",
        "    cnn.perf_eval()\n",
        "\n",
        "  t.join()\n",
        "  cnn.swap_data_buffers(\"TRAIN\")\n",
        "\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "eg-T_NQsN_VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate your model\n",
        "\n",
        "The following cell evaluates the accuracy and inference time over the test set.\n",
        "\n",
        "Colab usually puts the GPU into sleep mode after idling for a few seconds. Always run this cell a few times in a row to get the real execution time."
      ],
      "metadata": {
        "id": "quIkFmK1TLUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "python3 - <<EOF 2>&1 | tee out.txt\n",
        "\n",
        "import numpy as np\n",
        "from threading import Thread\n",
        "from helper import *\n",
        "import gc, time, sys, glob\n",
        "\n",
        "#Comment to access system wide install\n",
        "sys.path.insert(0,glob.glob('/content/CIANNA/src/build/lib.*/')[-1])\n",
        "import CIANNA as cnn\n",
        "\n",
        "def i_ar(int_list):\n",
        "  return np.array(int_list, dtype=\"int\")\n",
        "\n",
        "image_size = 32\n",
        "\n",
        "val_transform = A.Compose([]) #Images are alreay in the proper format, but create_validation_set expect an augm argument\n",
        "\n",
        "cnn.init(in_dim=i_ar([image_size,image_size]), in_nb_ch=3, out_dim=10,\n",
        "\tbias=0.1, b_size=16, comp_meth='C_CUDA', dynamic_load=1,\n",
        "\tmixed_precision=\"FP32C_FP32A\", adv_size=30)\n",
        "\n",
        "data_prep(0, image_size, test_mode=1)\n",
        "\n",
        "input_test, targets_test = create_validation_set(val_transform)\n",
        "cnn.create_dataset(\"TEST\", 10000, input_test[:,:], targets_test[:,:])\n",
        "gc.collect()\n",
        "\n",
        "load_epoch = 400\n",
        "cnn.load(\"net_save/net0_s%04d.dat\"%load_epoch, load_epoch, bin=1)\n",
        "\n",
        "\n",
        "#Run forward a first time to wake up the GPU and save the result\n",
        "cnn.forward(repeat=1, no_error=1, saving=2, drop_mode=\"AVG_MODEL\")\n",
        "\n",
        "start = time.perf_counter()\n",
        "#Run forward to evaluate raw model performance\n",
        "cnn.forward(no_error=1, saving=0, drop_mode=\"AVG_MODEL\")\n",
        "end = time.perf_counter()\n",
        "\n",
        "cnn.perf_eval()\n",
        "compute_time = (end-start)*1000 #in miliseconds\n",
        "print (\"Inference time: %f ms (%d ips)\"%(compute_time, int(10000/compute_time)))\n",
        "\n",
        "\n",
        "pred_results = np.fromfile(\"fwd_res/net0_%04d.dat\"%(load_epoch), dtype=\"float32\")\n",
        "pred_result = np.reshape(pred_results, (10000,-1))\n",
        "\n",
        "pred_correct = np.shape(np.where(np.argmax(pred_result[:,:10], axis=1) == np.argmax(targets_test[:,:], axis=1)))[1]\n",
        "pred_accuracy = (pred_correct/10000)*100.0\n",
        "print (\"Accuracy: %f, Error rate: %f\"%(pred_accuracy, 100.0-pred_accuracy))\n",
        "\n",
        "EOF"
      ],
      "metadata": {
        "id": "oMM45xmgTgFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
