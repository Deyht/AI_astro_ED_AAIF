{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ASIRRA base notebook for AI for Astrophysics AAIF Doctoral Course**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deyht/AI_astro_ED_AAIF/blob/main/practical_works/CNN/classification/ASIRRA/ASIRRA_CIANNA.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Link to the CIANNA github repository**\n",
        "https://github.com/Deyht/CIANNA"
      ],
      "metadata": {
        "id": "JfKCrIlDu-E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CIANNA installation**"
      ],
      "metadata": {
        "id": "vIXMFIFmvYzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query GPU allocation and properties\n",
        "\n",
        "If nvidia-smi fail, it might indicate that you launched the colab session whithout GPU reservation.  \n",
        "To change the type of reservation go to \"Runtime\"->\"Change runtime type\" and select \"GPU\" as your hardware accelerator."
      ],
      "metadata": {
        "id": "Ke8s2bCZvk1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi\n",
        "\n",
        "cd /content/\n",
        "\n",
        "git clone https://github.com/NVIDIA/cuda-samples/\n",
        "\n",
        "cd /content/cuda-samples/Samples/1_Utilities/deviceQuery/\n",
        "\n",
        "cmake CMakeLists.txt\n",
        "\n",
        "make SMS=\"50 60 70 80\"\n",
        "\n",
        "./deviceQuery | grep Capability | cut -c50- > ~/cuda_infos.txt\n",
        "./deviceQuery | grep \"CUDA Driver Version / Runtime Version\" | cut -c57- >> ~/cuda_infos.txt\n",
        "\n",
        "cd ~/"
      ],
      "metadata": {
        "id": "AHq06Uwk49Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are granted a GPU that supports high FP16 compute scaling (e.g the Tesla T4), it is advised to change the mixed_precision parameter in the prediction to \"FP16C_FP32A\".  \n",
        "See the detail description on mixed precision support with CIANNA on the [Systeme Requirements](https://github.com/Deyht/CIANNA/wiki/1\\)-System-Requirements) wiki page."
      ],
      "metadata": {
        "id": "tZ-lmHiRBFwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clone CIANNA git repository"
      ],
      "metadata": {
        "id": "A1SJ6-x8vqsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "git clone https://github.com/Deyht/CIANNA\n",
        "\n",
        "cd CIANNA"
      ],
      "metadata": {
        "id": "_uptvrov55YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compiling CIANNA for the allocated GPU generation\n",
        "\n",
        "There is no guaranteed forward or backward compatibility between Nvidia GPU generation, and some capabilities are generation specific. For these reasons, CIANNA must be provided the platform GPU generation at compile time.\n",
        "The following cell will automatically update all the necessary files based on the detected GPU, and compile CIANNA."
      ],
      "metadata": {
        "id": "JYGPC3OUv0td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/CIANNA\n",
        "\n",
        "mult=\"10\"\n",
        "cat ~/cuda_infos.txt\n",
        "comp_cap=\"$(sed '1!d' ~/cuda_infos.txt)\"\n",
        "cuda_vers=\"$(sed '2!d' ~/cuda_infos.txt)\"\n",
        "\n",
        "lim=\"11.1\"\n",
        "old_arg=$(awk '{if ($1 < $2) print \"-D CUDA_OLD\";}' <<<\"${cuda_vers} ${lim}\")\n",
        "\n",
        "sm_val=$(awk '{print $1*$2}' <<<\"${mult} ${comp_cap}\")\n",
        "\n",
        "gen_val=$(awk '{if ($1 >= 80) print \"-D GEN_AMPERE\"; else if($1 >= 70) print \"-D GEN_VOLTA\";}' <<<\"${sm_val}\")\n",
        "\n",
        "sed -i \"s/.*arch=sm.*/\\\\t\\tcuda_arg=\\\"\\$cuda_arg -D CUDA -D comp_CUDA -lcublas -lcudart -arch=sm_$sm_val $old_arg $gen_val\\\"/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" src/python_module_setup.py\n",
        "\n",
        "./compile.cp CUDA PY_INTERF\n",
        "\n",
        "mv src/build/lib.linux-x86_64-* src/build/lib.linux-x86_64"
      ],
      "metadata": {
        "id": "HGJUvmWW7YE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**IMPORTANT NOTE**   \n",
        "CIANNA is mainly used in a script fashion and was not designed to run in notebooks. Every cell code that directly invokes CIANNA functions must be run as a script to avoid possible errors.  \n",
        "To do so, the cell must have the following structure.\n",
        "\n",
        "```\n",
        "%%shell\n",
        "\n",
        "cd /content/CIANNA\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "[... your python code ...]\n",
        "\n",
        "EOF\n",
        "```\n",
        "\n",
        "This syntax allows one to easily edit python code in the notebook while running the cell as a script. Note that all the notebook variables can not be accessed by the cell in this context.\n"
      ],
      "metadata": {
        "id": "vbnBhbIL8wv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ASIRRA**\n",
        "\n",
        "The ASIRRA (Animal Species Image Recognition for Restricting Access) is a dataset that was originally used for CAPTCHA and HIP (Human Interactive Proofs).\n",
        "\n",
        "The original dataset comprises 25000 images of variable resolution (averaging around 350x500) and is equally distributed over the two classes \"Cat\" and \"Dog\". For this exercise, we provide two reduced versions in the form of padded and resized RGB images at either 128x128 or 256x256 as two binary files. This construction is necessary so the dataset can fit into the limited amount of Colab RAM. You can download one or both sets to test the impact of the input resolution on your network. In these files, the first 12500 images are of Cats, and the next 12500 are of Dogs. The last 1024 images of each class will be excluded to form our reference test dataset (total size 2048)."
      ],
      "metadata": {
        "id": "gd2waB3JYNkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading and visualizing the data\n",
        "\n",
        "We start by downloading and visualizing the raw data. You can get one or both of the resized versions."
      ],
      "metadata": {
        "id": "kULtlVy8Y5UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content\n",
        "\n",
        "wget https://share.obspm.fr/s/6TBsCpAASeETH3S/download/asirra_bin_128.tar.gz\n",
        "tar -xvzf asirra_bin_128.tar.gz\n",
        "\n",
        "#wget https://share.obspm.fr/s/52nxyfn7PjzawSe/download/asirra_bin_256.tar.gz\n",
        "#tar -xvzf asirra_bin_256.tar.gz"
      ],
      "metadata": {
        "id": "JvmqIq51H3-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "image_size = 128\n",
        "\n",
        "v_width = 8; v_height = 5\n",
        "nb_images = v_width*v_height\n",
        "\n",
        "f_im_s = image_size*image_size*3\n",
        "\n",
        "subset_cats = np.reshape(np.fromfile(\"asirra_bin_%d.dat\"%(image_size),\n",
        "  dtype=\"uint8\", count=f_im_s*(nb_images//2)), (nb_images//2,image_size,image_size,3))\n",
        "\n",
        "subset_dogs = np.reshape(np.fromfile(\"asirra_bin_%d.dat\"%(image_size),\n",
        "  dtype=\"uint8\", count=f_im_s*(nb_images//2), offset=12500*f_im_s), (nb_images//2,image_size,image_size,3))\n",
        "\n",
        "fig, ax = plt.subplots(v_height, v_width, figsize=(v_width*1.5,v_height*1.5), dpi=200, constrained_layout=True)\n",
        "\n",
        "for i in range(0, v_width*v_height):\n",
        "  c_x = i // v_width; c_y = i % v_width\n",
        "  p_c = int((i)%2) #Alternate cats and dogs in display\n",
        "  if(p_c == 0):\n",
        "    ax[c_x,c_y].imshow(subset_cats[i//2])\n",
        "  else:\n",
        "    ax[c_x,c_y].imshow(subset_dogs[i//2])\n",
        "  ax[c_x,c_y].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mjcrByRgYof3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data handling and augmentation\n",
        "\n",
        "To ease data manipulation and hyperparameter exploration, we first provide a set of helper functions. To make them accessible within the CIANNA script cells, we need to export them to a Python file. Every time you want to change the content of these functions, you will need to rerun the cell to generate a new .py file. If loaded in an interactive cell, you will need to restart the kernel after changing this file to re-import it properly."
      ],
      "metadata": {
        "id": "6ZeV1bvjRS4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, sys, gc, glob, time, cv2\n",
        "from threading import Thread\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "\n",
        "def data_prep(nb_images_per_iter, raw_image_size, image_size, test_mode=0):\n",
        "  #Data arrays are declared as global so we can work in place to reduce RAM footpring\n",
        "  global raw_images, input_data, targets, input_val, targets_val\n",
        "\n",
        "  raw_images = np.reshape(np.fromfile(\"asirra_bin_%d.dat\"%(raw_image_size), dtype=\"uint8\"), (25000, raw_image_size, raw_image_size,3))\n",
        "\n",
        "  if(test_mode == 0):\n",
        "    input_data = np.zeros((nb_images_per_iter,3*image_size**2), dtype=\"float32\") #CIANNA expects \"float32\" arrays\n",
        "    targets = np.zeros((nb_images_per_iter,2), dtype=\"float32\")\n",
        "\n",
        "  input_val = np.zeros((2048,3*image_size**2), dtype=\"float32\")\n",
        "  targets_val = np.zeros((2048,2), dtype=\"float32\")\n",
        "\n",
        "\n",
        "def create_augmented_batch(A_transform):\n",
        "\n",
        "  nb_images = np.shape(input_data)[0]\n",
        "\n",
        "  for i in range(0,nb_images):\n",
        "\n",
        "    l_class = np.random.randint(0,2)\n",
        "    l_id = np.random.randint(0,12500 - 1024)\n",
        "    #Last 1024 iamges of each class kept only for the val/test set\n",
        "\n",
        "    patch = raw_images[l_class*12500+l_id]\n",
        "    transformed = A_transform(image=patch)\n",
        "    patch_aug = transformed['image']\n",
        "\n",
        "    image_size = np.shape(patch_aug)[0]\n",
        "\n",
        "    #CIANNA expects data formated as 2D numpy arrays representing a list of flattened images (with every channel flattened after the others)\n",
        "    for depth in range(0,3): #We normalize based on mean pixel value\n",
        "      input_data[i,depth*image_size**2:(depth+1)*image_size**2] = (patch_aug[:,:,depth].flatten(\"C\") - 100.0)/155.0\n",
        "\n",
        "    targets[i,:] = 0.0\n",
        "    targets[i,l_class] = 1.0\n",
        "\n",
        "  return input_data, targets\n",
        "\n",
        "\n",
        "def create_validation_set(A_transform):\n",
        "\n",
        "  for i in range(0,2048):\n",
        "\n",
        "    l_class = i // 1024\n",
        "\n",
        "    patch = raw_images[(1+l_class)*(12500 - 1024) + i]\n",
        "    transformed = A_transform(image=patch)\n",
        "    patch_aug = transformed['image']\n",
        "\n",
        "    image_size = np.shape(patch_aug)[0]\n",
        "\n",
        "    for depth in range(0,3):\n",
        "      input_val[i,depth*image_size**2:(depth+1)*image_size**2] = (patch_aug[:,:,depth].flatten(\"C\") - 100.0)/155.0\n",
        "\n",
        "    targets_val[i,:] = 0.0\n",
        "    targets_val[i,l_class] = 1.0\n",
        "\n",
        "  return input_val, targets_val\n",
        "\n",
        "\n",
        "def free_data_helper():\n",
        "  global raw_images, input_data, targets, input_val, targets_val\n",
        "  del (raw_images, input_data, targets, input_val, targets_val)\n",
        "  return"
      ],
      "metadata": {
        "id": "HOKdPWXQLziA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test the helper functions with a simple example and display the produced images. The next cell illustrates how we can create an augmented batch of images for training from the raw image dataset.  \n",
        "\n",
        "Use this example to test the effect of combining different transform operations for data augmentation. You can also test the impact of the image resolution and of the position of the resize transformation in the augmentation list."
      ],
      "metadata": {
        "id": "bPo_XyxkXTy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper import *\n",
        "\n",
        "class_text = [\"cat\",\"dog\"]\n",
        "raw_image_size = 128\n",
        "image_size = 128\n",
        "\n",
        "v_width = 8; v_height = 5\n",
        "nb_images_per_iter = v_width*v_height\n",
        "\n",
        "#See Albumentation documentation for a list of existing augmentations\n",
        "train_transform = A.Compose([\n",
        "  #Image resize can be done after all other transform to preserve as much details as possible\n",
        "  #or as the fist operation so other transforms are faster\n",
        "  A.Resize(image_size,image_size, interpolation=2, p=1.0),\n",
        "  A.HorizontalFlip(p=0.5),\n",
        "  #Affine here act more as an aspect ratio transform than a scaling variation\n",
        "  A.Affine(scale=(0.3,1.3), translate_percent=(-0.3,0.3), rotate=(-10,10), interpolation=2, p=1.0),\n",
        "  A.ToGray(p=0.02),\n",
        "  A.ColorJitter(brightness=(0.8,1.2), contrast=(0.8,1.2), saturation=(0.8,1.2), hue=0.15, p=1.0),\n",
        "  ])\n",
        "\n",
        "val_transform = A.Compose([ #Here only a resize, but val transform could be more complex (center crop, padding, etc)\n",
        "  A.Resize(image_size, image_size, interpolation=2, p=1.0)])\n",
        "\n",
        "data_prep(nb_images_per_iter, raw_image_size, image_size)\n",
        "input_data, targets = create_augmented_batch(train_transform)\n",
        "\n",
        "fig, ax = plt.subplots(v_height, v_width, figsize=(v_width*1.5,v_height*1.5), dpi=200, constrained_layout=True)\n",
        "patch = np.zeros((image_size,image_size,3), dtype=\"uint8\")\n",
        "\n",
        "for i in range(0, v_width*v_height):\n",
        "  c_x = i // v_width; c_y = i % v_width\n",
        "  #Images in the augmented input_data array are directly in the CIANNA format.\n",
        "  #We need to convert them back to classical RGB for display.\n",
        "  for depth in range(0,3):\n",
        "    patch[:,:,depth] = np.reshape(input_data[i,depth*image_size**2:(depth+1)*image_size**2]*155 + 100,(image_size,image_size))\n",
        "\n",
        "  ax[c_x,c_y].imshow(patch)\n",
        "  ax[c_x,c_y].text(4, 10, class_text[(np.argmax(targets[i]))], c=\"red\", fontsize=10, clip_on=True)\n",
        "  ax[c_x,c_y].axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "free_data_helper()"
      ],
      "metadata": {
        "id": "yZYLSyJWXQ8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training a network\n",
        "\n",
        "The following cell allows to train a neural network architecture with CIANNA using dynamical augmentation through Albumentation. The architecture has been left empty as an exercice. Try to implement a woking architecture on this dataset and then try to achieve the highest possible accuracy.\n",
        "\n",
        "*Link to the [CIANNA](https://github.com/Deyht/CIANNA) repository. You can refer to CIANNA's [WIKI page](https://github.com/Deyht/CIANNA/wiki) for a complete framework description. You can also look at the full [API documentation](https://github.com/Deyht/CIANNA/wiki/4\\)-Interface-API-documentation) to add layer types that are absent from the LeNET-5 example.\n",
        "The saved models are available in the \"net_save\" repository that is automatically created when starting network training. The default naming scheme only refers to the training iteration, so rename your saving files with comprehensive information about your model to keep track of your progress. A saved model can be uploaded to a new Colab session for inference or further training.*"
      ],
      "metadata": {
        "id": "so4ypoJhLFkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd /content/\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "from helper import *\n",
        "\n",
        "sys.path.insert(0,glob.glob('/content/CIANNA/src/build/lib.*/')[-1])\n",
        "import CIANNA as cnn\n",
        "\n",
        "def i_ar(int_list):\n",
        "  return np.array(int_list, dtype=\"int\")\n",
        "\n",
        "raw_image_size = 128\n",
        "image_size = 128\n",
        "nb_images_per_iter = 4096 #Must likely be reduced if the image size is aumgented so examples can fit in RAM\n",
        "\n",
        "\n",
        "#See Albumentation documentation for a list of existing augmentations\n",
        "train_transform = A.Compose([\n",
        "  #Image resize can be done after all other transform to preserve as much details as possible\n",
        "  #or as the fist operation so other transforms are faster\n",
        "  A.Resize(image_size,image_size, interpolation=2, p=1.0),\n",
        "  A.HorizontalFlip(p=0.5),\n",
        "  #Affine here act more as an aspect ratio transform than a scaling variation\n",
        "  A.Affine(scale=(0.3,1.3), translate_percent=(-0.3,0.3), rotate=(-10,10), interpolation=2, p=1.0),\n",
        "  A.ToGray(p=0.02),\n",
        "  A.ColorJitter(brightness=(0.8,1.2), contrast=(0.8,1.2), saturation=(0.8,1.2), hue=0.15, p=1.0),\n",
        "  ])\n",
        "\n",
        "val_transform = A.Compose([ #Here only a resize, but val transform could be more complex (center crop, padding, etc)\n",
        "  A.Resize(image_size, image_size, interpolation=2, p=1.0)])\n",
        "\n",
        "\n",
        "#This funtion allow to launch data augmentation on a separate thread.\n",
        "#This way we can train on the GPU and generate new agumented examples in parallel.\n",
        "def data_augm():\n",
        "  input_data, targets = create_augmented_batch(train_transform)\n",
        "  cnn.delete_dataset(\"TRAIN_buf\", silent=1)\n",
        "  cnn.create_dataset(\"TRAIN_buf\", nb_images_per_iter, input_data[:,:], targets[:,:], silent=1)\n",
        "  return\n",
        "\n",
        "#In case the creation of new augmented data is too long compared to training, you can\n",
        "#increase the number of training iteration over a single augmentation\n",
        "nb_iter_per_augm = 2\n",
        "if(nb_iter_per_augm > 1):\n",
        "  shuffle_frequency = 1\n",
        "else:\n",
        "  shuffle_frequency = 0\n",
        "\n",
        "\n",
        "total_iter = 2000 #Should be increased with the complexity of the network and task\n",
        "load_iter = 0 #Used to reload a model at a given iteration\n",
        "\n",
        "if (len(sys.argv) > 1):\n",
        "  load_iter = int(sys.argv[1])\n",
        "\n",
        "start_iter = int(load_iter / nb_iter_per_augm)\n",
        "\n",
        "cnn.init(in_dim=i_ar([image_size,image_size]), in_nb_ch=3, out_dim=2,\n",
        "  bias=0.1, b_size=16, comp_meth='C_CUDA', dynamic_load=1,\n",
        "  mixed_precision=\"FP32C_FP32A\", adv_size=30)\n",
        "\n",
        "data_prep(nb_images_per_iter, raw_image_size, image_size)\n",
        "\n",
        "input_val, targets_val = create_validation_set(val_transform)\n",
        "cnn.create_dataset(\"VALID\", 2048, input_val[:,:], targets_val[:,:])\n",
        "cnn.create_dataset(\"TEST\", 2048, input_val[:,:], targets_val[:,:])\n",
        "del (input_val, targets_val) #The python arrays are no longer required after import in CIANNA\n",
        "gc.collect()\n",
        "\n",
        "#Create fist augmentation before parallelization\n",
        "input_data, targets = create_augmented_batch(train_transform)\n",
        "cnn.create_dataset(\"TRAIN\", nb_images_per_iter, input_data[:,:], targets[:,:])\n",
        "\n",
        "if(load_iter > 0):\n",
        "  cnn.load(\"net_save/net0_s%04d.dat\"%load_iter, load_iter, bin=1)\n",
        "else:\n",
        "  #Network backbone architecture\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=16  , padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=2, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=32  , padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=2, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=64\t, padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=4, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=128 , padding=i_ar([1,1]), activation=\"RELU\")\n",
        "  cnn.conv(f_size=i_ar([1,1]), nb_filters=64  , padding=i_ar([0,0]), activation=\"RELU\")\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=128 , padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=4, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=192 , padding=i_ar([1,1]), activation=\"RELU\")\n",
        "  cnn.conv(f_size=i_ar([1,1]), nb_filters=96  , padding=i_ar([0,0]), activation=\"RELU\")\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=192 , padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=8, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([1,1]), nb_filters=2 , padding=i_ar([0,0]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([1,1]), p_type=\"AVG\", p_global=1, activation=\"SMAX\")\n",
        "\n",
        "\n",
        "cnn.print_arch_tex(\"./arch/\", \"arch\", activation=1, dropout=1)\n",
        "\n",
        "for run_iter in range(start_iter,int(total_iter/nb_iter_per_augm)):\n",
        "\n",
        "  t = Thread(target=data_augm)\n",
        "  t.start()\n",
        "\n",
        "  cnn.train(nb_iter=nb_iter_per_augm, learning_rate=0.002, end_learning_rate=0.00003, shuffle_every=shuffle_frequency ,\\\n",
        "        control_interv=20, confmat=1, momentum=0.9, lr_decay=0.0012, weight_decay=0.001, save_every=20,\\\n",
        "        silent=0, save_bin=1, TC_scale_factor=256.0)\n",
        "\n",
        "  if(run_iter == start_iter):\n",
        "    cnn.perf_eval()\n",
        "\n",
        "  t.join()\n",
        "  cnn.swap_data_buffers(\"TRAIN\")\n",
        "\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "MvRhvumeRWZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate your model\n",
        "\n",
        "The following cell evaluates the accuracy and inference time over the test set.\n",
        "\n",
        "Colab usually puts the GPU into sleep mode after idling for a few seconds. Always run this cell a few times in a row to get the real execution time.\n",
        "\n"
      ],
      "metadata": {
        "id": "quIkFmK1TLUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "from helper import *\n",
        "\n",
        "#Comment to access system wide install\n",
        "sys.path.insert(0,glob.glob('/content/CIANNA/src/build/lib.*/')[-1])\n",
        "import CIANNA as cnn\n",
        "\n",
        "def i_ar(int_list):\n",
        "  return np.array(int_list, dtype=\"int\")\n",
        "\n",
        "raw_image_size = 128\n",
        "image_size = 128\n",
        "\n",
        "val_transform = A.Compose([ #Here only a resize, but val transform could be more complex (center crop, padding, etc)\n",
        "  A.Resize(image_size, image_size, interpolation=2, p=1.0)])\n",
        "\n",
        "cnn.init(in_dim=i_ar([image_size,image_size]), in_nb_ch=3, out_dim=2,\n",
        "  bias=0.1, b_size=512, comp_meth='C_CUDA', dynamic_load=1,\n",
        "  mixed_precision=\"FP16C_FP32A\", adv_size=30, inference_only=1)\n",
        "\n",
        "data_prep(0, raw_image_size, image_size, test_mode=1)\n",
        "\n",
        "#Compute on only half the validation set to reduce memory footprint\n",
        "input_test, targets_test = create_validation_set(val_transform)\n",
        "cnn.create_dataset(\"TEST\", 2048, input_test[:,:], targets_test[:,:])\n",
        "\n",
        "#Change epochs to load based on your own training\n",
        "load_epoch = 0\n",
        "if(load_epoch == 0):\n",
        "  #You can also download a pretrained model with the same architecture\n",
        "  os.system(\"wget https://share.obspm.fr/s/dyrJikKH2A7Sxso/download/arch2_res128_err1.76_ms89.dat\")\n",
        "  cnn.load(\"arch2_res128_err1.76_ms89.dat\", load_epoch, bin=1)\n",
        "else:\n",
        "  cnn.load(\"net_save/net0_s%04d.dat\"%load_epoch, load_epoch, bin=1)\n",
        "\n",
        "\n",
        "\n",
        "#Run forward a first time to wake up the GPU and save the result\n",
        "cnn.forward(repeat=1, no_error=1, saving=2, drop_mode=\"AVG_MODEL\")\n",
        "\n",
        "start = time.perf_counter()\n",
        "#Run forward to evaluate raw model performance\n",
        "cnn.forward(no_error=1, saving=0, drop_mode=\"AVG_MODEL\")\n",
        "end = time.perf_counter()\n",
        "\n",
        "cnn.perf_eval()\n",
        "compute_time = (end-start)*1000 #in miliseconds\n",
        "print (\"Inference time: %f ms (%d ips)\"%(compute_time, int(2048/compute_time)))\n",
        "\n",
        "\n",
        "pred_results = np.fromfile(\"fwd_res/net0_%04d.dat\"%(load_epoch), dtype=\"float32\")\n",
        "pred_result = np.reshape(pred_results, (2048,-1))\n",
        "\n",
        "pred_correct = np.shape(np.where(np.argmax(pred_result[:,:2], axis=1) == np.argmax(targets_test[:,:], axis=1)))[1]\n",
        "pred_accuracy = (pred_correct/2048)*100.0\n",
        "print (\"Accuracy: %f, Error rate: %f\"%(pred_accuracy, 100.0-pred_accuracy))\n",
        "\n",
        "EOF"
      ],
      "metadata": {
        "id": "oMM45xmgTgFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
